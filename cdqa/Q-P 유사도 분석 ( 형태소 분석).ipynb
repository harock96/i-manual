{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea24e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_name = 'i-manual-321006-6ae924a51dde.json'\n",
    "tokenizer_dic_name = 'add_tok'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878133f0",
   "metadata": {},
   "source": [
    "# 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b71fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "import json\n",
    "\n",
    "scope = [\n",
    "'https://spreadsheets.google.com/feeds',\n",
    "'https://www.googleapis.com/auth/drive',\n",
    "]\n",
    "\n",
    "credentials = ServiceAccountCredentials.from_json_keyfile_name(json_file_name, scope)\n",
    "gc = gspread.authorize(credentials)\n",
    "\n",
    "\n",
    "#client_email = 'data-load@i-manual-321006.iam.gserviceaccount.com'\n",
    "spreadsheet_url = 'https://docs.google.com/spreadsheets/d/1s36W9DHgnN0fBgZRLjxZOPOdGo21xJByz6cvMiljA4A/edit#gid=301631691'\n",
    "\n",
    "# 스프레스시트 문서 가져오기 \n",
    "doc = gc.open_by_url(spreadsheet_url)\n",
    "\n",
    "# 시트 선택하기\n",
    "worksheet1 = doc.worksheet('STRATEGY')\n",
    "worksheet2 = doc.worksheet('AUTHORITY')\n",
    "worksheet3 = doc.worksheet('TYPE')\n",
    "worksheet4 = doc.worksheet('DEFINITION')\n",
    "worksheet5 = doc.worksheet('CENTER')\n",
    "worksheet6 = doc.worksheet('PROFILE')\n",
    "\n",
    "from gspread_dataframe import get_as_dataframe, set_with_dataframe\n",
    "\n",
    "df1 = get_as_dataframe(worksheet1)\n",
    "df2 = get_as_dataframe(worksheet2)\n",
    "df3 = get_as_dataframe(worksheet3)\n",
    "df4 = get_as_dataframe(worksheet4)\n",
    "df5 = get_as_dataframe(worksheet5)\n",
    "df6 = get_as_dataframe(worksheet6)\n",
    "\n",
    "df1 = df1[:53]\n",
    "df2 = df2[:140]\n",
    "df3 = df3[:37]\n",
    "df4 = df4[:50]\n",
    "df5 = df5[:102]\n",
    "df6 = df6[:120]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a132b5d8",
   "metadata": {},
   "source": [
    "all_data : google spread sheet 에서 가져온 전체 데이터\n",
    "<br>anwers : 전체 데이터 중 'paragraph' 데이터\n",
    "<br>all_question_list : 전체 데이터 중 'question' 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26ae43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "all_data = pd.concat([df1,df2,df3,df4,df5,df6])\n",
    "all_data['paragraph'] = all_data['paragraph'].fillna(method = 'ffill')\n",
    "answers = all_data['paragraph'].tolist()\n",
    "\n",
    "all_question_list = all_data['question'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672dd6b2",
   "metadata": {},
   "source": [
    "## tag 붙이기\n",
    "all_questions : 전체 question에 tag 붙이기\n",
    "<br>all_answers : question에 대한 paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a20e2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_tag(name, title ,question):\n",
    "    if name != '':\n",
    "        return name + \" : \" + question\n",
    "    else:\n",
    "        return title + \" : \" + str(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad9b268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_tag_answer(name, title, answer):\n",
    "    if name != '':\n",
    "        return name + \" - \" + title + \" : \" + answer\n",
    "    else:\n",
    "        return title + \" : \" + answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35114ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question & Answer 뽑아내기\n",
    "all_questions = {}\n",
    "all_answers = {}\n",
    "all_qa_answers = {}\n",
    "\n",
    "df_list = [df1, df2, df3, df4, df5, df6]\n",
    "df_name = ['전략', '결정 방식', '종족', '에너지 흐름','','사회적 성향']\n",
    "\n",
    "for df, name in zip(df_list, df_name):\n",
    "    df['제목'] = df['제목'].fillna('')\n",
    "    df['paragraph_'] = df['paragraph'].fillna(method='ffill')\n",
    "\n",
    "    title = df['제목']\n",
    "    q = df['question']\n",
    "    a = df['paragraph_']\n",
    "    qa_ans = df['answer'].tolist()\n",
    "    \n",
    "    questions = []\n",
    "    answers_ = []\n",
    "    qa_answers = []\n",
    "    \n",
    "    for i in range(0, len(df)):\n",
    "        if title[i] != '':\n",
    "            if questions:\n",
    "                all_questions[t] = questions\n",
    "                questions = []\n",
    "                questions.append( concat_tag(name, title[i], q[i]) )\n",
    "            else:\n",
    "                all_questions[title[i]] = []\n",
    "                questions.append( concat_tag(name, title[i], q[i]) )\n",
    "                \n",
    "            if answers_ :\n",
    "                all_answers[t] = answers_\n",
    "                answers_ = []\n",
    "                answers_.append(concat_tag_answer(name, title[i], a[i]))\n",
    "            else:\n",
    "                all_answers[title[i]] = []\n",
    "                answers_.append(concat_tag_answer(name, title[i], a[i]))\n",
    "            \n",
    "            if qa_answers:\n",
    "                all_qa_answers[t] = qa_answers\n",
    "                qa_answers = []\n",
    "                qa_answers.append(qa_ans[i])\n",
    "            else:\n",
    "                all_qa_answers[title[i]] = []\n",
    "                qa_answers.append(qa_ans[i])\n",
    "            \n",
    "            t = title[i]\n",
    "        else:\n",
    "            questions.append( concat_tag(name, t, q[i]) )\n",
    "            answers_.append( concat_tag_answer(name, t, a[i]))\n",
    "            qa_answers.append(qa_ans[i])\n",
    "                                  \n",
    "    all_questions[t] = questions\n",
    "    all_answers[t] = answers_\n",
    "    all_qa_answers[t] = qa_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628dc716",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_qa_answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc88b1df",
   "metadata": {},
   "source": [
    "## i-Manual 통합 버전 데이터 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db205848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I-manual 통합 버전\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.concat([df1,df2,df3,df4,df5,df6])\n",
    "df_new = df.drop(['내용','answer_start_pos','id', 'Unnamed: 7', 'Unnamed: 8', 'Unnamed: 9',\n",
    "       'Unnamed: 10', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13',\n",
    "       'Unnamed: 14', 'Unnamed: 15', 'Unnamed: 16', 'Unnamed: 17',\n",
    "       'Unnamed: 18', 'Unnamed: 19', 'Unnamed: 20', 'Unnamed: 21',\n",
    "       'Unnamed: 22', 'Unnamed: 23', 'Unnamed: 24', 'Unnamed: 25'], axis=1)\n",
    "df = df.drop(['내용', 'answer','answer_start_pos','question','id', 'Unnamed: 7', 'Unnamed: 8', 'Unnamed: 9',\n",
    "       'Unnamed: 10', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13',\n",
    "       'Unnamed: 14', 'Unnamed: 15', 'Unnamed: 16', 'Unnamed: 17',\n",
    "       'Unnamed: 18', 'Unnamed: 19', 'Unnamed: 20', 'Unnamed: 21',\n",
    "       'Unnamed: 22', 'Unnamed: 23', 'Unnamed: 24', 'Unnamed: 25'], axis=1)\n",
    "\n",
    "df = df.dropna()\n",
    "df = df.reset_index()\n",
    "df = df.replace('', np.NaN)\n",
    "df = df.fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba196937",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3c5f4e",
   "metadata": {},
   "source": [
    "### 전체 데이터 tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2ddca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tags = list(dict.fromkeys(df['제목'].tolist()))\n",
    "\n",
    "all_paragraph_tags = []\n",
    "for i in range(0, len(all_tags)):\n",
    "    if i < 5:\n",
    "        all_paragraph_tags.append(\"전략 - \"+all_tags[i])\n",
    "    elif i < 13 :\n",
    "        all_paragraph_tags.append(\"결정 방식 - \"+all_tags[i])\n",
    "    elif i < 18:\n",
    "        all_paragraph_tags.append(\"종족 - \"+all_tags[i])\n",
    "    elif i < 23:\n",
    "        all_paragraph_tags.append(\"에너지 흐름 - \"+all_tags[i])\n",
    "    elif 40 < i :\n",
    "        all_paragraph_tags.append(\"사회적 성향 - \"+all_tags[i])\n",
    "    else :\n",
    "        all_paragraph_tags.append(all_tags[i])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f182011d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_paragraphs = []\n",
    "for tag, tag_ in zip(all_tags, all_paragraph_tags):\n",
    "    index = df.index[df['제목']==tag].tolist()\n",
    "    for i in index:\n",
    "        all_paragraphs.append(tag_+\" : \"+df.loc[i, 'paragraph'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b16ddf",
   "metadata": {},
   "source": [
    "### 내담자 tag (예시) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3965c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = ['가이드', \n",
    "       '은둔자/은근 사교가 (2/4)', \n",
    "       '두 묶음 흐름', \n",
    "       '감정 결정방식', \n",
    "       '가이드의 전략',\n",
    "       '연료센터(DEFINED)',      \n",
    "       '활력센터(UNDEFINED)',\n",
    "       '직관센터(UNDEFINED)',\n",
    "       '감정센터(DEFINED)',\n",
    "       '에고센터(DEFINED)',\n",
    "       '방향센터(UNDEFINED)',\n",
    "       '생각센터(DEFINED)',\n",
    "       '영감센터(DEFINED)',\n",
    "       '표현센터(DEFINED)'\n",
    "      ]\n",
    "\n",
    "paragraph_tags = ['종족 - 가이드', \n",
    "       '사회적 성향 - 은둔자/은근 사교가(2/4)', \n",
    "       '에너지 흐름 - 두 묶음 흐름', \n",
    "       '결정 방식 - 감정 결정방식', \n",
    "       '전략 - 가이드의 전략',\n",
    "       '연료센터(DEFINED)',      \n",
    "       '활력센터(UNDEFINED)',\n",
    "       '직관센터(UNDEFINED)',\n",
    "       '감정센터(DEFINED)',\n",
    "       '에고센터(DEFINED)',\n",
    "       '방향센터(UNDEFINED)',\n",
    "       '생각센터(DEFINED)',\n",
    "       '영감센터(DEFINED)',\n",
    "       '표현센터(DEFINED)'\n",
    "      ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d536416",
   "metadata": {},
   "source": [
    "## 내담자별 정보 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c800008",
   "metadata": {},
   "source": [
    "### CONTEXT - 내담자의 정보 리스트 (Paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6ddaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = []\n",
    "for tag, tag_ in zip(tags, paragraph_tags):\n",
    "    index = df.index[df['제목']==tag].tolist()\n",
    "    for i in index:\n",
    "        paragraphs.append(tag_+\" : \"+df.loc[i, 'paragraph'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7166340a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01fa263",
   "metadata": {},
   "outputs": [],
   "source": [
    "#paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d818dff6",
   "metadata": {},
   "source": [
    "### Question - 내담자의 정보에 대한 질문 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2665a2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "personal_questions = {}\n",
    "for tag in tags:\n",
    "    personal_questions[tag] = all_questions[tag]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec2d6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#personal_questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbe529a",
   "metadata": {},
   "source": [
    "### Answer - 내담자의 정보에 대한 정답 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425dec9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "personal_answers = {}\n",
    "for tag in tags:\n",
    "    personal_answers[tag] = all_answers[tag]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f26932",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#personal_answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191ce7a5",
   "metadata": {},
   "source": [
    "### QA Answer - 내담자 정보에 대한 QA 정답 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7085c9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "personal_qa_answers = {}\n",
    "for tag in tags:\n",
    "    personal_qa_answers[tag] = all_qa_answers[tag]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec49519",
   "metadata": {},
   "outputs": [],
   "source": [
    "#personal_qa_answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825af6e9",
   "metadata": {},
   "source": [
    "# 2. Tokenizer\n",
    "직접 학습시킨 tokenizer 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de33eb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "#tokenizer = BertTokenizer.from_pretrained('songhee/i-manual-mbert')\n",
    "tokenizer = BertTokenizer.from_pretrained(tokenizer_dic_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ed67fa",
   "metadata": {},
   "source": [
    "### tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d045fdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_tokenizer(text):\n",
    "    inputs = tokenizer(text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "    text_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    return text_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1295a56f",
   "metadata": {},
   "source": [
    "### 불용어 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd025d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_words(stop_words, text_list):\n",
    "    \"\"\"불용어 제거\"\"\"\n",
    "    stop_words = stop_words.split(' ')\n",
    "    \n",
    "    # merge morphs to words\n",
    "    edit = []\n",
    "    \n",
    "    for text in text_list:\n",
    "        tmp = []\n",
    "        for token in text:\n",
    "            if token not in stop_words:\n",
    "                tmp.append(token)\n",
    "\n",
    "        edit.append(tmp)\n",
    "\n",
    "    return edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26237456",
   "metadata": {},
   "source": [
    "### 단어로 합치기(##제거)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24071bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_morphs(text_list):\n",
    "    \"\"\"\n",
    "    단어로 합치기(##제거)\n",
    "    \"\"\"\n",
    "    edit = []\n",
    "    \n",
    "    for text in text_list:\n",
    "        tmp = []\n",
    "        for i in range (0, len(text)) :\n",
    "            if text[i][:2] == '##' :\n",
    "                a = []\n",
    "                a.append(tmp[-1])\n",
    "                a.append(text[i][2:])\n",
    "                tmp[-1] = ''.join(a)\n",
    "            else :\n",
    "                tmp.append(text[i])\n",
    "\n",
    "        edit.append(tmp)\n",
    "    \n",
    "    return edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e93558",
   "metadata": {},
   "source": [
    "### paragraph tokenize 함수\n",
    "1. text 토큰화\n",
    "2. 토큰화된 상태에서 불용어 제거 (morphs)\n",
    "3. 토큰화된 상태를 단어로 합치기\n",
    "4. 단어 상태의 불용어 제거 (words)\n",
    "5. 분리된 단어를 문장으로 합치기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db13478",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenized_text(text_list):\n",
    "    \n",
    "    # tokenized paragraphs\n",
    "    tokenized_paragraphs = []\n",
    "\n",
    "    for text in text_list:\n",
    "        tokenized_paragraphs.append(my_tokenizer(text))\n",
    "    \n",
    "    # remove stop words(morphs)\n",
    "    stop_words = '##은 ##을 ##를 ##에 ##의 ##이 ##으로 ##로 ##에게 ##것' ###입 ##니다 ##습 ##합\n",
    "    tokenized_paragraphs_edit = remove_words(stop_words, tokenized_paragraphs)\n",
    "    \n",
    "    # merge morphs to words\n",
    "    tokenized_paragraphs_edit = merge_morphs(tokenized_paragraphs_edit)\n",
    "    #tokenized_paragraphs_edit = merge_morphs(tokenized_paragraphs)\n",
    "    \n",
    "    # remove stop words(words)\n",
    "    stop_words = \"입니다 합니다 있습니다 또한 것입니다 그리고 또는 것이 것을 의 ' : ? ! !! ?' !' , . #\" + '\"'\n",
    "    tokenized_paragraphs_edit = remove_words(stop_words, tokenized_paragraphs_edit)\n",
    "    \n",
    "    return tokenized_paragraphs_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323df864",
   "metadata": {},
   "source": [
    "### Tokenize paragraph + 역토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7deb493",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = tokenized_text(paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3c91df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detokenized(tokenized_corpus):\n",
    "    # merge words to sentences\n",
    "    \"\"\"분리된 단어를 문장으로 합치기\"\"\"\n",
    "    tokenized_paragraphs = []\n",
    "    stop_words = \"입니다 합니다 있습니다 또한 것입니다 그리고 또는 것이 것을 의 ' : ? ! !! ?' !' , . #\" + '\"'\n",
    "\n",
    "    for i in range(0, len(tokenized_corpus)):\n",
    "        sentence = \"\"\n",
    "        for w in tokenized_corpus[i] :\n",
    "            if w not in stop_words :\n",
    "                sentence += w\n",
    "                sentence += \" \"\n",
    "        tokenized_paragraphs.append(sentence)\n",
    "    return tokenized_paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7a0e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_paragraphs = detokenized(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80e58c2",
   "metadata": {},
   "source": [
    "### 띄어쓰기 보정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479d2cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_white_space(answer):\n",
    "\n",
    "    if not answer:\n",
    "        return answer\n",
    "\n",
    "    # 작은 따옴표의 개수에 따라 시작 위치 변경\n",
    "    toggle_c = answer.count(\"'\")\n",
    "    \n",
    "    if toggle_c and toggle_c % 2 == 0 :  #짝수개일 경우 시작부분부터 시작  카운트\n",
    "        toggle = True\n",
    "    else :\n",
    "        toggle = False      #홀수개일 경우 끝부분부터 시작 카운트\n",
    "   \n",
    "    tokens = answer.split()\n",
    "    l_space = [\"‘\"]  # 다음 토큰에 붙어야 하는 토큰\n",
    "    r_space = [\",\", \".\",\"!\",\"?\",\")\", \"~\",\"%\"] # 이전 토큰에 붙어야하는 토큰\n",
    "    numbers = [\"0\", \"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"]\n",
    "    n_space = [\"(\",\"/\",\"’\"] # 앞뒤 토큰 모두 붙어야 하는 토큰\n",
    "    length = len(tokens)\n",
    "\n",
    "    result = []\n",
    "    result.append(tokens[0])\n",
    "    l_s = False\n",
    "    for i in range(1, length):\n",
    "         \n",
    "        # 앞 뒤로 다 붙어야 하는 경우\n",
    "        if tokens[i] in n_space:\n",
    "            result[-1] = result[-1] + tokens[i]\n",
    "            l_s = True\n",
    "            continue\n",
    "        \n",
    "        if (tokens[i-1] ==\"~\" or tokens[i-1]==\".\") and tokens[i][0] in numbers:\n",
    "            result[-1] = result[-1] + tokens[i]\n",
    "            continue\n",
    "\n",
    "        if (tokens[i-1] == \"'\" and toggle) or (tokens[i-1]==\")\" and len(tokens[i])==1):\n",
    "            result[-1] = result[-1] + tokens[i]\n",
    "            continue\n",
    "        \n",
    "        # 다음 토큰에 붙어야 하는 경우\n",
    "        if tokens[i] in l_space or (tokens[i] == \"'\" and toggle):\n",
    "            l_s = True\n",
    "            \n",
    "            result.append(tokens[i])\n",
    "            \n",
    "            if tokens[i] == \"'\":\n",
    "                toggle = False\n",
    "\n",
    "            continue\n",
    "        \n",
    "        # 앞 토큰에 붙어야 하는 경우\n",
    "        if (tokens[i] in r_space) or l_s or (tokens[i] == \"'\" and toggle==False):\n",
    "            result[-1] = result[-1] + tokens[i]\n",
    "            l_s = False\n",
    "\n",
    "            if tokens[i] == \"'\":\n",
    "                toggle = True\n",
    "            \n",
    "            continue\n",
    "        \n",
    "        result.append(tokens[i])\n",
    "        \n",
    "\n",
    "    return \" \".join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2488427",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_paragraphs_edit = []\n",
    "for text in tokenized_paragraphs:\n",
    "    tokenized_paragraphs_edit.append(remove_white_space(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eba9d90",
   "metadata": {},
   "source": [
    "### 전체 데이터에 대한 paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc182a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokenized_corpus = tokenized_text(all_paragraphs)\n",
    "# 역 토큰화\n",
    "all_tokenized_paragraphs = detokenized(all_tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfc02fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokenized_paragraphs_edit = []\n",
    "for text in all_tokenized_paragraphs:\n",
    "    all_tokenized_paragraphs_edit.append(remove_white_space(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ecf82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_tokenized_paragraphs_edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febe4c30",
   "metadata": {},
   "source": [
    "# 3. TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d2631b",
   "metadata": {},
   "source": [
    "### 유사도 구하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423b600d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def get_similar_paragraphs(context, question, top_n=3, original_context=None):\n",
    "    \"\"\"\n",
    "    context : 내담자의 전체 paragraphs를 토큰화한 결과\n",
    "    original_context : 내담자의 전체 paragraphs (원본)\n",
    "    question : 내담자 질문\n",
    "    top_n : 1~n위의 유사도 결과 추출\n",
    "    \"\"\"\n",
    "    \n",
    "    if original_context is None:\n",
    "        original = context\n",
    "    else:\n",
    "        original = original_context\n",
    "    \n",
    "    tfidf_vectorizer = TfidfVectorizer(min_df=1)\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(context+[question])\n",
    "    doc_similarities = (tfidf_matrix * tfidf_matrix.T)\n",
    "    \n",
    "    #top_similar = similarities.argsort()[-(top_n+1):][::-1][1:]\n",
    "    question_similarities = doc_similarities.toarray()[-1][:-1]\n",
    "    top_similar = question_similarities.argsort()[::-1]\n",
    "    \n",
    "    output = {\n",
    "            \"question\" : question,\n",
    "            \"top_similar_paragraphs\": [{\n",
    "                    \"paragraphs\": context[similar_idx],\n",
    "                    \"original_paragraphs\" : original[similar_idx],\n",
    "                    \"similarity\": round(question_similarities[similar_idx], 6)\n",
    "            } for similar_idx in top_similar]\n",
    "        }\n",
    "    \n",
    "    return output   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a77fa8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15dbbc56",
   "metadata": {},
   "source": [
    "# 형태소 분석 (무현 실험 분담)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76a52df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "\n",
    "def mecab_normalize(text_list):\n",
    "    ###################################################################\n",
    "    # 실질 형태소 품사\n",
    "    meaning_tags = [\"NNG\", \"NNP\", \"NNB\", \"NR\", \"NP\", \"VV\", \"VA\", \"VX\", \"VCP\", \"VCN\", \"MM\", \"MAG\", \"MAJ\", \"IC\"]\n",
    "    \n",
    "    # 실질 형태소 + 어근, 접미사, 접두사\n",
    "    meaning_tags = [\"NNG\", \"NNP\", \"NNB\", \"NR\", \"NP\", \"VV\", \"VA\", \"VX\", \"VCP\", \"VCN\", \"MM\", \"MAG\", \"MAJ\", \"IC\", \"XR\", \"XPN\", \"XSN\", \"XSV\", \"XSA\"]\n",
    "    \n",
    "    ########### 둘중 원하는 방식으로 사용하면 됩니다. ##################\n",
    "    \n",
    "    # 딕셔너리 파일 위치는 사용자 별로 상이\n",
    "    m = Mecab('C:\\\\mecab\\\\mecab-ko-dic')\n",
    "    \n",
    "    meaning_text = []\n",
    "    for text in text_list:\n",
    "        out = m.pos(text)\n",
    "\n",
    "        # 실질 형태소만 담는 배열\n",
    "        meaning_words = []\n",
    "\n",
    "        for word in out:\n",
    "            # word의 형태는 (\"단어\", \"품사\") 이므로 word[1]은 품사를 나타내고 word[0]는 단어를 나타냄.\n",
    "            if word[1] in meaning_tags:\n",
    "                meaning_words.append(word[0])\n",
    "        meaning_text.append(meaning_words)\n",
    "    return meaning_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46033d4",
   "metadata": {},
   "source": [
    "## 4-1 내담자 Paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c712c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 1\n",
    "context = tokenized_paragraphs_edit\n",
    "\n",
    "correct = 0\n",
    "incorrect = 0\n",
    "a_c = 0\n",
    "a_i = 0\n",
    "result_dic = {'tag':[], '정답':[], '오답':[]}\n",
    "\n",
    "for tag in tags:\n",
    "    tokenized_questions = mecab_normalize(personal_questions[tag])\n",
    "    tokenized_questions = detokenized(tokenized_questions)\n",
    "    #tokenized_questions = all_questions[tag]\n",
    "\n",
    "    answer_list = personal_answers[tag]\n",
    "    answer_list = tokenized_text(answer_list)\n",
    "    answer_list = detokenized(answer_list)\n",
    "    \n",
    "    answer_list = [ remove_white_space(text) for text in answer_list]\n",
    "\n",
    "    \n",
    "    for answer, question in zip(answer_list, tokenized_questions) :\n",
    "        results = get_similar_paragraphs(context, question, top_n)\n",
    "\n",
    "        #print(results['question'], \"\\n\")\n",
    "\n",
    "        if results['top_similar_paragraphs'][0]['similarity'] > 0 :\n",
    "            #prediction = results['top_similar_paragraphs'][i]['paragraphs'].replace(tag_+ \" : \", '')\n",
    "            prediction = results['top_similar_paragraphs'][0]['paragraphs']\n",
    "\n",
    "            #print(prediction, \"\\n\")\n",
    "            #print(answer, \"\\n\")\n",
    "\n",
    "            if prediction == answer :\n",
    "                correct +=1 \n",
    "                #print(\"==================================정답!\\n\")\n",
    "            else :\n",
    "                incorrect +=1\n",
    "                #print(\"==================================오답!\\n\")\n",
    "\n",
    "    result_dic['tag'].append(tag)\n",
    "    result_dic['정답'].append(correct)\n",
    "    result_dic['오답'].append(incorrect)\n",
    "    \n",
    "    a_c += correct\n",
    "    a_i += incorrect\n",
    "    \n",
    "    correct =0\n",
    "    incorrect=0\n",
    "    \n",
    "result_df = pd.DataFrame(result_dic, columns=['tag','정답','오답'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1201b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9811f60b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44950e33",
   "metadata": {},
   "source": [
    "## 4-2 내담자 종족, 결정 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb6577b",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 1\n",
    "context = tokenized_paragraphs_edit\n",
    "setting = [['가이드', 0,2], ['감정 결정방식',4,6]]\n",
    "\n",
    "for tag, start, end in setting :\n",
    "    c = context[start:end]\n",
    "    \n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "\n",
    "    tokenized_questions = mecab_normalize(personal_questions[tag])\n",
    "    tokenized_questions = detokenized(tokenized_questions)\n",
    "    #tokenized_questions = all_questions[tag]\n",
    "\n",
    "    answer_list = personal_answers[tag]\n",
    "    answer_list = tokenized_text(answer_list)\n",
    "    answer_list = detokenized(answer_list)\n",
    "\n",
    "    answer_list = [ remove_white_space(text) for text in answer_list]\n",
    "\n",
    "    for answer, question in zip(answer_list, tokenized_questions) :\n",
    "        results = get_similar_paragraphs(c, question, top_n)\n",
    "\n",
    "        #print(results['question'], \"\\n\")\n",
    "\n",
    "        if results['top_similar_paragraphs'][0]['similarity'] > 0 :\n",
    "            #prediction = results['top_similar_paragraphs'][i]['paragraphs'].replace(tag_+ \" : \", '')\n",
    "            prediction = results['top_similar_paragraphs'][0]['paragraphs']\n",
    "\n",
    "            #print(prediction, \"\\n\")\n",
    "            #print(answer, \"\\n\")\n",
    "\n",
    "            if prediction == answer :\n",
    "                correct +=1 \n",
    "                #print(\"==================================정답!\\n\")\n",
    "            else :\n",
    "                incorrect +=1\n",
    "                #print(\"==================================오답!\\n\")\n",
    "    print(\"\\n----\",tag,\"----\\n\")\n",
    "    print(\"정답: \",correct)\n",
    "    print(\"오답: \",incorrect)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
